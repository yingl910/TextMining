{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association Mining:Paradigmatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements paradigmatic word association mining.Two words have paradigmatic relation if they can be\n",
    "substituted for each other. In other words, they are in the same semantic class, or syntactic class and replacement wouldnâ€™t infect understanding of sentences.In this notebook, I just work on Nouns.\n",
    "\n",
    "**Method**:\n",
    "1. Represent each noun by a list of nouns that ever appear in the same sentences as this noun (context)\n",
    "2. Convert each word list into tf-idf vector and compute pairwise consine similarity\n",
    "3. Nouns with high context similarity likely have paradigmatic relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "import en_core_web_sm\n",
    "from gensim.parsing.preprocessing import STOPWORDS as stop\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from MongoDB & Parse Reviews into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Variable\n",
    "db: database to be used\n",
    "review_collection: collection to be use\n",
    "'''\n",
    "\n",
    "'''AWS connection'''\n",
    "\n",
    "# client = MongoClient(\"\", 27017)\n",
    "# client.the_database.authenticate('','', mechanism='', source='')\n",
    "# db = client['nike_collections_legacy']\n",
    "# review_collection = db['nike_reviews_trial']\n",
    "\n",
    "'''local connection'''\n",
    "\n",
    "client = MongoClient(\"localhost\", 27018)\n",
    "client.the_database.authenticate('','', mechanism='', source='')\n",
    "db = client['nike_collections']\n",
    "review_collection = db['dev_clean_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazonR3RBSGXB60GJCM</td>\n",
       "      <td>I have a really hard time finding shoes that f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eastbay30301564</td>\n",
       "      <td>I purchased these for my nephew who says they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazonRY7EM3S8LZ7RA</td>\n",
       "      <td>Cheap shoe. Don't buy it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazonR2FUQ0XY7TOIDG</td>\n",
       "      <td>Runs 1/2 size large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazonR2STVGL1OF7IKG</td>\n",
       "      <td>Great shoes and very comfortable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id                                        review_text\n",
       "0  amazonR3RBSGXB60GJCM  I have a really hard time finding shoes that f...\n",
       "1       eastbay30301564  I purchased these for my nephew who says they ...\n",
       "2   amazonRY7EM3S8LZ7RA                           Cheap shoe. Don't buy it\n",
       "3  amazonR2FUQ0XY7TOIDG                                Runs 1/2 size large\n",
       "4  amazonR2STVGL1OF7IKG                   Great shoes and very comfortable"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sample\n",
    "\n",
    "review_data = review_collection.aggregate(\n",
    "    [{'$sample': {'size':10000}},\n",
    "     {'$project': {'_id': 1, 'review_text':1}}\n",
    "    ])\n",
    "\n",
    "reviews = pd.DataFrame(list(review_data))\n",
    "reviews.head()\n",
    "\n",
    "#use population\n",
    "\n",
    "# review_data = review_collection.find({})\n",
    "# reviews = pd.DataFrame(list(review_data))\n",
    "# reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load spacy nlp pipeline\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''sentence parser'''\n",
    "sentence = []\n",
    "for index, row in reviews.iterrows():\n",
    "    if row['review_text'] is not None:\n",
    "        #spacy sentence parsing exception handling\n",
    "        review = row['review_text'].replace('|','.') \n",
    "        review = re.sub('\\(|\\)',' ',review)\n",
    "        review = re.sub('!+','.',review)\n",
    "        review = re.sub('[ ]*![ ]*','.',review)\n",
    "        review = re.sub('\\.\\.+','.',review)\n",
    "        review = re.sub('-*','',review) \n",
    "        tokens = nlp(review) \n",
    "        for sen in tokens.sents:\n",
    "            s = re.sub('^[^a-zA-z]*|[^a-zA-Z]*$','',sen.text)\n",
    "            if s!='':\n",
    "                sentence.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencce Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_lemma = []\n",
    "for sent in sentence:\n",
    "    doc = nlp(sent)\n",
    "    for word in doc:\n",
    "        if word.pos_ == 'NOUN' and word.lemma_ not in stop:\n",
    "            tokens_lemma.append(word.lemma_)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_lemma_count = Counter(tokens_lemma)\n",
    "freq_noun = {k: v for k, v in tokens_lemma_count.items() if v > 10}\n",
    "candidate_noun = list(freq_noun.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Acquisition\n",
    "context level: sentence <br/>\n",
    "context type: left, right (adjacent); general (non-adjacent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_context =defaultdict(list)\n",
    "right_context = defaultdict(list)\n",
    "general_context = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split word and their context \n",
    "def index_context(dictionary):\n",
    "    list_index = []\n",
    "    list_text = []\n",
    "    for key,value in dictionary.items():\n",
    "        list_index.append(key)\n",
    "        list_text.append(value)\n",
    "    return (list_index,list_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in sentence:\n",
    "    doc = nlp(sent)\n",
    "    if len(doc)> 1:\n",
    "        for word in doc:\n",
    "            if word.pos_ == 'NOUN' and word.lemma_ in candidate_noun:\n",
    "                if word.i == 0:\n",
    "                    right_context[word.text].append(doc[1].text)\n",
    "                    general_context[word.text].extend([token.text for token in doc[2:]])\n",
    "                elif word.i == len(doc)-1:\n",
    "                    left_context[word.text].append(doc[word.i-1].text)\n",
    "                    general_context[word.text].extend([token.text for token in doc[:word.i-1]])\n",
    "                else:   \n",
    "                    left_context[word.text].append(doc[word.i-1].text)\n",
    "                    right_context[word.text].append(doc[word.i+1].text)\n",
    "                    general_context[word.text].extend([token.text for token in doc if token.i not in list(range(word.i-1,word.i+2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_index,left_text = index_context(left_context)\n",
    "right_index,right_text = index_context(right_context)\n",
    "general_index, general_text = index_context(general_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word dictionary\n",
    "left_dictionary = corpora.Dictionary(left_text)\n",
    "right_dictionary = corpora.Dictionary(right_text)\n",
    "general_dictionary = corpora.Dictionary(general_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf vector\n",
    "left_corpus = [left_dictionary.doc2bow(text) for text in left_text]\n",
    "right_corpus = [right_dictionary.doc2bow(text) for text in right_text]\n",
    "general_corpus = [general_dictionary.doc2bow(text) for text in general_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf_idf models\n",
    "left_tfidf = models.TfidfModel(left_corpus)\n",
    "right_tfidf = models.TfidfModel(right_corpus)\n",
    "general_tfidf = models.TfidfModel(general_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf_idf vector\n",
    "left_corpus_tfidf = [left_tfidf[i] for i in left_corpus]\n",
    "right_corpus_tfidf = [right_tfidf[i] for i in right_corpus]\n",
    "general_corpus_tfidf = [general_tfidf[i] for i in general_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Pairwise Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(corpus):\n",
    "    result = {}\n",
    "    doc = similarities.MatrixSimilarity(corpus)\n",
    "    count = 0\n",
    "    for i in corpus:\n",
    "        sims = doc[i] \n",
    "        result[count] = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        count += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_sim = compute_similarity(left_corpus_tfidf)\n",
    "right_sim = compute_similarity(right_corpus_tfidf)\n",
    "general_sim = compute_similarity(general_corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(context_list,index_list,word):\n",
    "    temp = {}\n",
    "    for pair in context_list:\n",
    "        if pair[1]>=threshold:\n",
    "            word_temp = index_list[pair[0]]\n",
    "            if word_temp != word:\n",
    "                temp[word_temp] = pair[1]\n",
    "    return sorted(temp.items(), key=operator.itemgetter(1),reverse=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(general_sim)):\n",
    "    word = general_index[i]\n",
    "    glist = general_sim[i]\n",
    "    \n",
    "    try:\n",
    "        llist = left_sim[left_index.index(word)]\n",
    "    except:\n",
    "        llist = None\n",
    "    try:\n",
    "        rlist = right_sim[right_index.index(word)]\n",
    "    except:\n",
    "        rlist = None\n",
    "        \n",
    "    item = {}\n",
    "    item['word'] = word\n",
    "    item['left_context'] = []\n",
    "    item['right_context'] = []\n",
    "    item['general_context'] = []\n",
    "\n",
    "    if llist:\n",
    "        item['left_context'] = get_context(llist,left_index,word)\n",
    "\n",
    "    if rlist:\n",
    "        item['right_context'] = get_context(rlist,right_index,word)\n",
    "\n",
    "    if glist:\n",
    "        item['general_context'] = get_context(glist,general_index,word)\n",
    "\n",
    "    output.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "with open('paradigmatic_word_association.csv', 'w',encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, output[0].keys())\n",
    "    w.writeheader()\n",
    "    for i in output:\n",
    "        w.writerow(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
