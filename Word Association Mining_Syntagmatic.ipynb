{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association Mining:Syntagmatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements syntagmatic word association mining.Two words have syntagmatic relation if they can be\n",
    "combined with each other. \n",
    "\n",
    "**Method**:\n",
    "<br/>I use the idea of mutual information, words with high co-occurrences but relatively low individual occurrences. In this notebook, I work on noun&noun and adj&noun pairs.I also applied smoothing technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Variable\n",
    "db: database to be used\n",
    "review_collection: collection to be use\n",
    "'''\n",
    "\n",
    "'''AWS connection'''\n",
    "\n",
    "# client = MongoClient(\"\", 27017)\n",
    "# client.the_database.authenticate('','', mechanism='', source='')\n",
    "# db = client['nike_collections_legacy']\n",
    "# review_collection = db['nike_reviews_trial']\n",
    "\n",
    "'''local connection'''\n",
    "\n",
    "client = MongoClient(\"localhost\", 27018)\n",
    "client.the_database.authenticate('','', mechanism='', source='')\n",
    "db = client['nike_collections']\n",
    "review_collection = db['dev_clean_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazonR24P1S9UXJDBT8</td>\n",
       "      <td>Great No problems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazonR8VRWEFCI638E</td>\n",
       "      <td>These shoes are fabulous.  I ordered 1/2 size ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kohls131578626</td>\n",
       "      <td>If you play disc golf this is it ! This shoe i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dicks166977931</td>\n",
       "      <td>This is my second pair of these exact shoes.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazonR33S9L65EQBL9B</td>\n",
       "      <td>Fits great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id                                        review_text\n",
       "0  amazonR24P1S9UXJDBT8                                  Great No problems\n",
       "1   amazonR8VRWEFCI638E  These shoes are fabulous.  I ordered 1/2 size ...\n",
       "2        kohls131578626  If you play disc golf this is it ! This shoe i...\n",
       "3        dicks166977931  This is my second pair of these exact shoes.  ...\n",
       "4  amazonR33S9L65EQBL9B                                        Fits great."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sample\n",
    "\n",
    "review_data = review_collection.aggregate(\n",
    "    [{'$sample': {'size':10000}},\n",
    "     {'$project': {'_id': 1, 'review_text':1}}\n",
    "    ])\n",
    "\n",
    "reviews = pd.DataFrame(list(review_data))\n",
    "reviews.head()\n",
    "\n",
    "#use population\n",
    "\n",
    "# review_data = review_collection.find({})\n",
    "# reviews = pd.DataFrame(list(review_data))\n",
    "# reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load spacy nlp pipeline\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''sentence parser''' \n",
    "sentence = []\n",
    "for index, row in reviews.iterrows():\n",
    "    if row['review_text'] is not None:\n",
    "        #spacy sentence parsing exception handling\n",
    "        review = row['review_text'].replace('|','.') \n",
    "        review = re.sub('\\(|\\)',' ',review)\n",
    "        review = re.sub('!+','.',review)\n",
    "        review = re.sub('[ ]*![ ]*','.',review)\n",
    "        review = re.sub('\\.\\.+','.',review)\n",
    "        review = re.sub('-*','',review) \n",
    "        tokens = nlp(review) \n",
    "        for sen in tokens.sents:\n",
    "            s = re.sub('^[^a-zA-z]*|[^a-zA-Z]*$','',sen.text)\n",
    "            if s!='':\n",
    "                sentence.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of segment: sentence\n",
    "sen_num = len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count: Individual Word & Word Pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_text = {}\n",
    "pair_text = {}\n",
    "\n",
    "for sent in sentence:\n",
    "    doc = nlp(sent)\n",
    "    noun = []\n",
    "    for word in doc:\n",
    "        if (word.pos_ == 'NOUN' or word.pos_ == 'ADJ') and word.lemma_ not in stop:\n",
    "            noun.append(word.text)\n",
    "    # duplicate removal: what we want is the number of sentences each word appear \n",
    "    # not the actual word count\n",
    "    noun = list(set(noun))\n",
    "\n",
    "    for n in noun:\n",
    "        tokens_text[n] = tokens_text.get(n,0) + 1\n",
    "        for y in noun[noun.index(n)+1:]:\n",
    "            key = frozenset((n,y))\n",
    "            pair_text[key] = pair_text.get(key,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smoothing \n",
    "# filter out words that appear less than 11 times\n",
    "p_w1_1 = {k: (v+0.5)/(sen_num+1) for k, v in tokens_text.items() if v>10}\n",
    "p_w1_0 = {k: 1-v for k, v in p_w1_1.items()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all word pairs that don't appear in dataset, give them a probability 0 for future smoothing\n",
    "words = list(p_w1_1.keys())\n",
    "for w1 in words:\n",
    "    for w2 in words[words.index(w1)+1:]:\n",
    "        if frozenset((w1,w2)) not in pair_text:\n",
    "            pair_text[frozenset((w1,w2))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smoothing\n",
    "p_w1_1_w2_1 = {k: (v+0.25)/(sen_num+1) for k, v in pair_text.items()}\n",
    "p_w1_1_w2_0 = {}\n",
    "p_w1_0_w2_0 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate P(w1=1,w2=0), P(w1=0,w2=1) and P(w1=0,w2=0) for all word pairs\n",
    "for w1 in words:\n",
    "    for w2 in words[words.index(w1)+1:]:\n",
    "        # P(w1=1)\n",
    "        pw1 = p_w1_1[w1]\n",
    "        # P(w2=1)\n",
    "        pw2 = p_w1_1[w2]\n",
    "        k10 = (w1,w2)\n",
    "        k01 = (w2,w1)\n",
    "        # P(w1=1,w2=1)\n",
    "        p11 = p_w1_1_w2_1[frozenset(k10)]\n",
    "        # P(w1=1,w2=0)\n",
    "        p_w1_1_w2_0[k10] = pw1 - p11\n",
    "        # P(w1=0,w2=1)\n",
    "        p_w1_1_w2_0[k01] = pw2 - p11\n",
    "        if frozenset(k10) not in p_w1_0_w2_0:\n",
    "            # P(w1=0,w2=0)\n",
    "            p_w1_0_w2_0[frozenset(k10)] = p_w1_0[w1] - p_w1_1_w2_0[k01]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mutual_info: key: word pair; value: MI calculated using KL divergence \n",
    "mutual_info = {}\n",
    "for w1 in words:\n",
    "    for w2 in words[words.index(w1)+1:]:\n",
    "        k10 = (w1,w2)\n",
    "        k01 = (w2,w1)\n",
    "        if frozenset(k10) not in mutual_info:\n",
    "            # 1,1\n",
    "            pw1_1 = p_w1_1[w1]\n",
    "            pw2_1 = p_w1_1[w2] \n",
    "            p11 = p_w1_1_w2_1[frozenset(k10)]\n",
    "            a = p11 * math.log2(p11/(pw1_1*pw2_1))\n",
    "            \n",
    "            # 0,0\n",
    "            pw1_0 = p_w1_0[w1]\n",
    "            pw2_0 = p_w1_0[w2]\n",
    "            p00 = p_w1_0_w2_0[frozenset(k10)]\n",
    "            b = p00 * math.log2(p00/(pw1_0*pw2_0))\n",
    "            \n",
    "            p10 = p_w1_1_w2_0[k10]\n",
    "            p01 = p_w1_1_w2_0[k01]\n",
    "            c = p10 * math.log2(p10/(pw1_1*pw2_0))\n",
    "            d = p01 * math.log2(p01/(pw1_0*pw2_1))\n",
    "            mutual_info[frozenset(k10)] = a + b + c + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Threshold (to filter result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to select threshold\n",
    "a = np.array(list(mutual_info.values()))\n",
    "total_num = len(mutual_info)\n",
    "print('Total Number: ',total_num)\n",
    "print('Threshold for Top 100: ',np.percentile(a, (total_num-100)/total_num*100))\n",
    "print('Threshold for Top 150: ',np.percentile(a, (total_num-150)/total_num*100))\n",
    "print('Threshold for Top 200: ',np.percentile(a, (total_num-200)/total_num*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi_threshold = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter result based on MI value \n",
    "mutual_info_filter = {k: v for k, v in mutual_info.items() if v > mi_threshold}\n",
    "# sort based on MI value\n",
    "mutual_info_sorted = sorted(mutual_info_filter.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the result into dictionary with master word labeling \n",
    "MI = []\n",
    "for item in mutual_info_sorted:\n",
    "    result = {}\n",
    "    words = item[0]\n",
    "    for i,w in enumerate(words):\n",
    "        name1 = 'word_' + str(i)\n",
    "        result[name1] = w\n",
    "    result['mutual_information'] = item[1]\n",
    "    MI.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('syntagmatic_word_association.csv', 'w',encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, MI[0].keys())\n",
    "    w.writeheader()\n",
    "    for i in MI:\n",
    "        w.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
